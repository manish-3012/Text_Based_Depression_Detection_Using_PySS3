The fuck does America actually get right? All over r/AskReddit and stuff there are these treads of â€œwhat does America get wrongâ€ or â€œwhat questions do you have for Americansâ€ am that almost always spiral into just ripping everything about America to shreds. Like I get it, America is fucked up. Our president sucks, the two party political system is one of the dumbest things in existence, there are way to many morbidly obese people, there is no free health care for whatever reason but like, there has to be something that America gets right, right? 

And before anyone says anything, no, Iâ€™m not some â€œFUCKING AMERICA IS THE BEST FREEEEEEDOOOOMMM!!!!!!!â€ type of person. Iâ€™m just curious. What does America do right?