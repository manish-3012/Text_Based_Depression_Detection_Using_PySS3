Why do people keep saying things get better?On every thread I see there are people urging others not to do it because life gets better. It might for some people but for some people it doesnâ€™t. You can learn new skills to cope and manage your life better but when it comes to finding love and friendships and community, that is mostly out of your control. And for some people that never happens. Why would people lie about that? Iâ€™m just wondering if people want to share their thoughts. It seems cruel to give people false hope, but maybe my perspective is skewed.