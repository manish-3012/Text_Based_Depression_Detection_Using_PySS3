Why the fuck do schools only promote college I literally went through high school with teachers only talking about college. I pretty much feel like itâ€™s brainwashing at this fucking point, they act like itâ€™s the only option to succeed. Theyâ€™re literally setting kids up for failure, school isnâ€™t for everyone and the education system acting like youâ€™re defective if itâ€™s not is fucking appalling. Why not at least introduce trades as another option instead of acting like they donâ€™t exist? Not to mention the crippling debt. Fuck this Iâ€™m dropping out lol.