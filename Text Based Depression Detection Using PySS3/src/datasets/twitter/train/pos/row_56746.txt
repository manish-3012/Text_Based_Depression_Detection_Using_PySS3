My mom said that we need to stop using the word "Karen" She said that somebody on Facebook said it was a sexist and sometimes racist term made to insult women. I need a definition of what a Karen is (no swear words cuz I'm gonna tell her).

So far I've come up with

- stuck up

- think they own the world and everyone owes them something

What else?